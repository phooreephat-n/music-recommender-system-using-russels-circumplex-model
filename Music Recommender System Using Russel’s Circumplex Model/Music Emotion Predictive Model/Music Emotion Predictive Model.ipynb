{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df_with_lyrics = pd.read_csv('/song_df_with_lyrics.csv')\n",
    "song_df_with_lyrics = song_df_with_lyrics.drop_duplicates(subset='spotify_id').reset_index(drop=True)\n",
    "\n",
    "audio_features_df = pd.read_csv('/song_df_with_audio_features.csv')\n",
    "audio_features_df = audio_features_df.drop_duplicates(subset='spotify_id').reset_index(drop=True)\n",
    "\n",
    "merge_df = pd.merge(song_df_with_lyrics, audio_features_df, on='spotify_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign target label\n",
    "label_mapping = {'High Valence High Arousal': 0, 'Low Valence High Arousal': 1, 'Low Valence Low Arousal': 2, 'High Valence Low Arousal': 3}\n",
    "merge_df['target'] = merge_df['Mood Quadrant'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spotify_id</th>\n",
       "      <th>normalized_lyrics</th>\n",
       "      <th>Mood Quadrant</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>key_7</th>\n",
       "      <th>key_8</th>\n",
       "      <th>key_9</th>\n",
       "      <th>key_10</th>\n",
       "      <th>key_11</th>\n",
       "      <th>mode_0</th>\n",
       "      <th>mode_1</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32xfcxu2gKRVmDopzlmnUc</td>\n",
       "      <td>cool side cool side cool side cool side the co...</td>\n",
       "      <td>Low Valence High Arousal</td>\n",
       "      <td>0.462436</td>\n",
       "      <td>0.249437</td>\n",
       "      <td>0.645942</td>\n",
       "      <td>0.830321</td>\n",
       "      <td>0.236994</td>\n",
       "      <td>0.328553</td>\n",
       "      <td>0.101148</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ocORq8GJBUAIJdi8QPgme</td>\n",
       "      <td>yeah yeah im a virgin yeah i stay lurkin stay ...</td>\n",
       "      <td>Low Valence Low Arousal</td>\n",
       "      <td>0.627085</td>\n",
       "      <td>0.207180</td>\n",
       "      <td>0.631470</td>\n",
       "      <td>0.788153</td>\n",
       "      <td>0.374672</td>\n",
       "      <td>0.251345</td>\n",
       "      <td>0.105012</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.450</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2bhwPUsgts9pmZIbMvlHZV</td>\n",
       "      <td>hey kid wan na hear something bug when the kni...</td>\n",
       "      <td>High Valence Low Arousal</td>\n",
       "      <td>0.540944</td>\n",
       "      <td>0.858137</td>\n",
       "      <td>0.808191</td>\n",
       "      <td>0.047892</td>\n",
       "      <td>0.065160</td>\n",
       "      <td>0.488848</td>\n",
       "      <td>0.136614</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4xIlIbQDXw9BXVFcBdPrvW</td>\n",
       "      <td>a day in falsettoland dr mendel at work you go...</td>\n",
       "      <td>Low Valence Low Arousal</td>\n",
       "      <td>0.544215</td>\n",
       "      <td>0.487886</td>\n",
       "      <td>0.696775</td>\n",
       "      <td>0.036747</td>\n",
       "      <td>0.459800</td>\n",
       "      <td>0.209176</td>\n",
       "      <td>0.115004</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.300</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1X5DB8JnEDPiPskaxLIbfk</td>\n",
       "      <td>one one two three four every second i have ill...</td>\n",
       "      <td>High Valence High Arousal</td>\n",
       "      <td>0.583470</td>\n",
       "      <td>0.849082</td>\n",
       "      <td>0.813579</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.056647</td>\n",
       "      <td>0.485716</td>\n",
       "      <td>0.107991</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               spotify_id                                  normalized_lyrics  \\\n",
       "0  32xfcxu2gKRVmDopzlmnUc  cool side cool side cool side cool side the co...   \n",
       "1  1ocORq8GJBUAIJdi8QPgme  yeah yeah im a virgin yeah i stay lurkin stay ...   \n",
       "2  2bhwPUsgts9pmZIbMvlHZV  hey kid wan na hear something bug when the kni...   \n",
       "3  4xIlIbQDXw9BXVFcBdPrvW  a day in falsettoland dr mendel at work you go...   \n",
       "4  1X5DB8JnEDPiPskaxLIbfk  one one two three four every second i have ill...   \n",
       "\n",
       "               Mood Quadrant  danceability    energy  loudness  acousticness  \\\n",
       "0   Low Valence High Arousal      0.462436  0.249437  0.645942      0.830321   \n",
       "1    Low Valence Low Arousal      0.627085  0.207180  0.631470      0.788153   \n",
       "2   High Valence Low Arousal      0.540944  0.858137  0.808191      0.047892   \n",
       "3    Low Valence Low Arousal      0.544215  0.487886  0.696775      0.036747   \n",
       "4  High Valence High Arousal      0.583470  0.849082  0.813579      0.003052   \n",
       "\n",
       "   liveness     tempo  duration_ms  ...  key_7  key_8  key_9  key_10  key_11  \\\n",
       "0  0.236994  0.328553     0.101148  ...      0      0      0       0       1   \n",
       "1  0.374672  0.251345     0.105012  ...      0      0      0       0       0   \n",
       "2  0.065160  0.488848     0.136614  ...      0      1      0       0       0   \n",
       "3  0.459800  0.209176     0.115004  ...      0      0      1       0       0   \n",
       "4  0.056647  0.485716     0.107991  ...      0      0      0       0       0   \n",
       "\n",
       "   mode_0  mode_1  valence  arousal  target  \n",
       "0       0       1   -0.150    0.075       1  \n",
       "1       1       0   -0.450   -0.250       2  \n",
       "2       0       1    0.225   -0.175       3  \n",
       "3       0       1   -0.075   -0.300       2  \n",
       "4       1       0    0.400    0.075       0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "\n",
    "# Input Features\n",
    "X_lyrics = merge_df['normalized_lyrics']\n",
    "X_audio = np.array(merge_df[['danceability', 'energy', 'loudness', 'acousticness', 'liveness', 'tempo', 'duration_ms', 'key_0', 'key_1', 'key_2', 'key_3', 'key_4', 'key_5', 'key_6', 'key_7', 'key_8', 'key_9', 'key_10', 'key_11', 'mode_0', 'mode_1']])\n",
    "\n",
    "# Target Feature\n",
    "y = merge_df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Only Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: SVM\n",
      "F1-score average: 0.5164544406994128\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.73      0.57      0.64       485\n",
      " Low Valence High Arousal       0.19      0.46      0.27       110\n",
      "  Low Valence Low Arousal       0.61      0.36      0.45       253\n",
      " High Valence Low Arousal       0.30      0.45      0.36       108\n",
      "\n",
      "                 accuracy                           0.49       956\n",
      "                macro avg       0.46      0.46      0.43       956\n",
      "             weighted avg       0.59      0.49      0.52       956\n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "F1-score average: 0.7489490788075812\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.77      0.91      0.83       485\n",
      " Low Valence High Arousal       0.77      0.48      0.59       110\n",
      "  Low Valence Low Arousal       0.76      0.76      0.76       253\n",
      " High Valence Low Arousal       0.64      0.40      0.49       108\n",
      "\n",
      "                 accuracy                           0.76       956\n",
      "                macro avg       0.74      0.64      0.67       956\n",
      "             weighted avg       0.76      0.76      0.75       956\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "F1-score average: 0.7504472243531213\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.76      0.94      0.84       485\n",
      " Low Valence High Arousal       0.90      0.35      0.50       110\n",
      "  Low Valence Low Arousal       0.74      0.78      0.76       253\n",
      " High Valence Low Arousal       0.90      0.41      0.56       108\n",
      "\n",
      "                 accuracy                           0.77       956\n",
      "                macro avg       0.83      0.62      0.67       956\n",
      "             weighted avg       0.79      0.77      0.75       956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_audio, y, test_size=0.2, random_state=13)\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    (\"SVM\", SVC(random_state=13, class_weight='balanced', kernel='rbf')),\n",
    "    (\"Random Forest\", RandomForestClassifier(random_state=13)),\n",
    "    (\"XGBoost\", XGBClassifier(random_state=13))\n",
    "]\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models:\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model   \n",
    "    f1 = f1_score(y_test, y_pred, average='weighted') \n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"F1-score average: {f1}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['High Valence High Arousal', 'Low Valence High Arousal', 'Low Valence Low Arousal', 'High Valence Low Arousal']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Only Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define n-grams configurations\n",
    "ngram_configs = [\n",
    "    (1, 1),  # Unigram\n",
    "    (2, 2),  # Bigram\n",
    "    (3, 3)  # Trigram\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with n-grams: (1, 1)\n",
      "F1-score average: 0.7184591099851467\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.90      0.62      0.74       485\n",
      " Low Valence High Arousal       0.61      0.87      0.72       110\n",
      "  Low Valence Low Arousal       0.72      0.72      0.72       253\n",
      " High Valence Low Arousal       0.48      0.94      0.63       108\n",
      "\n",
      "                 accuracy                           0.71       956\n",
      "                macro avg       0.68      0.79      0.70       956\n",
      "             weighted avg       0.77      0.71      0.72       956\n",
      "\n",
      "\n",
      "Training with n-grams: (2, 2)\n",
      "F1-score average: 0.7703806350853118\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.93      0.69      0.79       485\n",
      " Low Valence High Arousal       0.64      0.94      0.76       110\n",
      "  Low Valence Low Arousal       0.79      0.76      0.77       253\n",
      " High Valence Low Arousal       0.54      0.96      0.70       108\n",
      "\n",
      "                 accuracy                           0.77       956\n",
      "                macro avg       0.72      0.84      0.75       956\n",
      "             weighted avg       0.81      0.77      0.77       956\n",
      "\n",
      "\n",
      "Training with n-grams: (3, 3)\n",
      "F1-score average: 0.6616240165506182\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.94      0.57      0.71       485\n",
      " Low Valence High Arousal       0.33      0.87      0.48       110\n",
      "  Low Valence Low Arousal       0.71      0.62      0.66       253\n",
      " High Valence Low Arousal       0.54      0.74      0.62       108\n",
      "\n",
      "                 accuracy                           0.64       956\n",
      "                macro avg       0.63      0.70      0.62       956\n",
      "             weighted avg       0.76      0.64      0.66       956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "for ngram_range in ngram_configs:\n",
    "    print(f\"\\nTraining with n-grams: {ngram_range}\")\n",
    "    \n",
    "    # Use TfidfVectorizer for feature extraction\n",
    "    if ngram_range != (3, 3):\n",
    "        vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_df=0.09, min_df=88)\n",
    "        text_features = vectorizer.fit_transform(X_lyrics).toarray()\n",
    "\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_df=0.5, min_df=32)\n",
    "        text_features = vectorizer.fit_transform(X_lyrics).toarray()\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(text_features, y, test_size=0.2, random_state=13)\n",
    "\n",
    "    # Create a pipeline with vectorizer and LinearSVC\n",
    "    model = make_pipeline(StandardScaler(with_mean=False), SVC(max_iter=1000, random_state=13, class_weight='balanced', kernel='rbf'))\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"F1-score average: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['High Valence High Arousal', 'Low Valence High Arousal', 'Low Valence Low Arousal', 'High Valence Low Arousal']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with n-grams: (1, 1)\n",
      "F1-score average: 0.7763712755522691\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.76      0.98      0.85       485\n",
      " Low Valence High Arousal       0.78      0.56      0.65       110\n",
      "  Low Valence Low Arousal       0.85      0.68      0.75       253\n",
      " High Valence Low Arousal       0.98      0.44      0.61       108\n",
      "\n",
      "                 accuracy                           0.79       956\n",
      "                macro avg       0.84      0.67      0.72       956\n",
      "             weighted avg       0.81      0.79      0.78       956\n",
      "\n",
      "\n",
      "Training with n-grams: (2, 2)\n",
      "F1-score average: 0.7600093442135\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.75      0.98      0.85       485\n",
      " Low Valence High Arousal       0.69      0.60      0.64       110\n",
      "  Low Valence Low Arousal       0.87      0.61      0.72       253\n",
      " High Valence Low Arousal       0.98      0.42      0.58       108\n",
      "\n",
      "                 accuracy                           0.78       956\n",
      "                macro avg       0.82      0.65      0.70       956\n",
      "             weighted avg       0.80      0.78      0.76       956\n",
      "\n",
      "\n",
      "Training with n-grams: (3, 3)\n",
      "F1-score average: 0.6881802784122794\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.70      0.94      0.80       485\n",
      " Low Valence High Arousal       0.61      0.46      0.53       110\n",
      "  Low Valence Low Arousal       0.78      0.54      0.64       253\n",
      " High Valence Low Arousal       0.78      0.32      0.46       108\n",
      "\n",
      "                 accuracy                           0.71       956\n",
      "                macro avg       0.72      0.57      0.61       956\n",
      "             weighted avg       0.72      0.71      0.69       956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "for ngram_range in ngram_configs:\n",
    "    print(f\"\\nTraining with n-grams: {ngram_range}\")\n",
    "    \n",
    "    # Use TfidfVectorizer for feature extraction\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_df=0.95, min_df=30)\n",
    "    text_features = vectorizer.fit_transform(X_lyrics).toarray()\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(text_features, y, test_size=0.2, random_state=13)\n",
    "\n",
    "    # Create a pipeline with vectorizer and LinearSVC\n",
    "    model = make_pipeline(StandardScaler(with_mean=False), RandomForestClassifier(random_state=13))\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"F1-score average: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['High Valence High Arousal', 'Low Valence High Arousal', 'Low Valence Low Arousal', 'High Valence Low Arousal']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with n-grams: (1, 1)\n",
      "F1-score average: 0.7599003267575425\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.72      0.99      0.83       485\n",
      " Low Valence High Arousal       0.97      0.51      0.67       110\n",
      "  Low Valence Low Arousal       0.88      0.63      0.74       253\n",
      " High Valence Low Arousal       0.96      0.42      0.58       108\n",
      "\n",
      "                 accuracy                           0.78       956\n",
      "                macro avg       0.88      0.64      0.70       956\n",
      "             weighted avg       0.82      0.78      0.76       956\n",
      "\n",
      "\n",
      "Training with n-grams: (2, 2)\n",
      "F1-score average: 0.6709326101819102\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.66      1.00      0.79       485\n",
      " Low Valence High Arousal       0.90      0.25      0.40       110\n",
      "  Low Valence Low Arousal       0.84      0.52      0.64       253\n",
      " High Valence Low Arousal       0.94      0.31      0.47       108\n",
      "\n",
      "                 accuracy                           0.71       956\n",
      "                macro avg       0.84      0.52      0.58       956\n",
      "             weighted avg       0.77      0.71      0.67       956\n",
      "\n",
      "\n",
      "Training with n-grams: (3, 3)\n",
      "F1-score average: 0.49489450310781646\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.56      0.99      0.72       485\n",
      " Low Valence High Arousal       1.00      0.10      0.18       110\n",
      "  Low Valence Low Arousal       0.76      0.21      0.32       253\n",
      " High Valence Low Arousal       0.78      0.13      0.22       108\n",
      "\n",
      "                 accuracy                           0.58       956\n",
      "                macro avg       0.78      0.36      0.36       956\n",
      "             weighted avg       0.69      0.58      0.49       956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "for ngram_range in ngram_configs:\n",
    "    print(f\"\\nTraining with n-grams: {ngram_range}\")\n",
    "    \n",
    "    # Use TfidfVectorizer for feature extraction\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_df=0.95, min_df=30)\n",
    "    text_features = vectorizer.fit_transform(X_lyrics).toarray()\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(text_features, y, test_size=0.2, random_state=13)\n",
    "\n",
    "    # Create a pipeline with vectorizer and LinearSVC\n",
    "    model = make_pipeline(StandardScaler(with_mean=False), XGBClassifier(random_state=13))\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"F1-score average: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['High Valence High Arousal', 'Low Valence High Arousal', 'Low Valence Low Arousal', 'High Valence Low Arousal']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Audio + Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define n-grams configurations\n",
    "ngram_configs = [\n",
    "    (1, 1),  # Unigram\n",
    "    (2, 2),  # Bigram\n",
    "    (3, 3)  # Trigram\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with n-grams: (1, 1)\n",
      "F1-score average: 0.7617930931025326\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.87      0.77      0.82       485\n",
      " Low Valence High Arousal       0.55      0.85      0.67       110\n",
      "  Low Valence Low Arousal       0.82      0.66      0.73       253\n",
      " High Valence Low Arousal       0.58      0.81      0.67       108\n",
      "\n",
      "                 accuracy                           0.76       956\n",
      "                macro avg       0.70      0.77      0.72       956\n",
      "             weighted avg       0.79      0.76      0.76       956\n",
      "\n",
      "\n",
      "Training with n-grams: (2, 2)\n",
      "F1-score average: 0.7774082425131245\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.87      0.77      0.81       485\n",
      " Low Valence High Arousal       0.53      0.85      0.65       110\n",
      "  Low Valence Low Arousal       0.83      0.70      0.76       253\n",
      " High Valence Low Arousal       0.70      0.87      0.77       108\n",
      "\n",
      "                 accuracy                           0.77       956\n",
      "                macro avg       0.73      0.80      0.75       956\n",
      "             weighted avg       0.80      0.77      0.78       956\n",
      "\n",
      "\n",
      "Training with n-grams: (3, 3)\n",
      "F1-score average: 0.7531550893122987\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.88      0.74      0.80       485\n",
      " Low Valence High Arousal       0.48      0.84      0.61       110\n",
      "  Low Valence Low Arousal       0.84      0.67      0.74       253\n",
      " High Valence Low Arousal       0.58      0.85      0.69       108\n",
      "\n",
      "                 accuracy                           0.74       956\n",
      "                macro avg       0.70      0.77      0.71       956\n",
      "             weighted avg       0.79      0.74      0.75       956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "for ngram_range in ngram_configs:\n",
    "    print(f\"\\nTraining with n-grams: {ngram_range}\")\n",
    "    \n",
    "       # Use TfidfVectorizer for feature extraction\n",
    "    if ngram_range != (3, 3):\n",
    "        vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_df=0.09, min_df=92)\n",
    "        text_features = vectorizer.fit_transform(X_lyrics).toarray()\n",
    "\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_df=0.5, min_df=32)\n",
    "        text_features = vectorizer.fit_transform(X_lyrics).toarray()\n",
    "    \n",
    "    # Combine Audio and Text Features\n",
    "    combined_features = np.hstack([X_audio, text_features])\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=13)\n",
    "\n",
    "    # Create a pipeline with vectorizer and LinearSVC\n",
    "    model = make_pipeline(StandardScaler(with_mean=False), SVC(random_state=13, class_weight='balanced', kernel='rbf'))\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"F1-score average: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['High Valence High Arousal', 'Low Valence High Arousal', 'Low Valence Low Arousal', 'High Valence Low Arousal']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with n-grams: (1, 1)\n",
      "F1-score average: 0.7799180706612663\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.78      0.98      0.87       485\n",
      " Low Valence High Arousal       0.67      0.56      0.61       110\n",
      "  Low Valence Low Arousal       0.83      0.69      0.75       253\n",
      " High Valence Low Arousal       1.00      0.44      0.62       108\n",
      "\n",
      "                 accuracy                           0.79       956\n",
      "                macro avg       0.82      0.67      0.71       956\n",
      "             weighted avg       0.81      0.79      0.78       956\n",
      "\n",
      "\n",
      "Training with n-grams: (2, 2)\n",
      "F1-score average: 0.7875925613728345\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.79      0.96      0.87       485\n",
      " Low Valence High Arousal       0.63      0.53      0.57       110\n",
      "  Low Valence Low Arousal       0.83      0.73      0.78       253\n",
      " High Valence Low Arousal       0.98      0.49      0.65       108\n",
      "\n",
      "                 accuracy                           0.80       956\n",
      "                macro avg       0.81      0.68      0.72       956\n",
      "             weighted avg       0.81      0.80      0.79       956\n",
      "\n",
      "\n",
      "Training with n-grams: (3, 3)\n",
      "F1-score average: 0.7917561130920306\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.80      0.97      0.88       485\n",
      " Low Valence High Arousal       0.62      0.53      0.57       110\n",
      "  Low Valence Low Arousal       0.85      0.75      0.80       253\n",
      " High Valence Low Arousal       0.96      0.45      0.62       108\n",
      "\n",
      "                 accuracy                           0.80       956\n",
      "                macro avg       0.81      0.68      0.72       956\n",
      "             weighted avg       0.81      0.80      0.79       956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RF\n",
    "for ngram_range in ngram_configs:\n",
    "    print(f\"\\nTraining with n-grams: {ngram_range}\")\n",
    "    \n",
    "    # Use TfidfVectorizer for feature extraction\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_df=0.95, min_df=80)\n",
    "    text_features = vectorizer.fit_transform(X_lyrics).toarray()\n",
    "    \n",
    "    # Combine Audio and Text Features\n",
    "    combined_features = np.hstack([X_audio, text_features])\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=13)\n",
    "\n",
    "    # Create a pipeline with vectorizer and LinearSVC\n",
    "    model = make_pipeline(StandardScaler(with_mean=False), RandomForestClassifier(n_estimators=1000, random_state=13))\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"F1-score average: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['High Valence High Arousal', 'Low Valence High Arousal', 'Low Valence Low Arousal', 'High Valence Low Arousal']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with n-grams: (1, 1)\n",
      "F1-score average: 0.7388014980377501\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.76      0.94      0.84       485\n",
      " Low Valence High Arousal       0.95      0.36      0.53       110\n",
      "  Low Valence Low Arousal       0.71      0.77      0.74       253\n",
      " High Valence Low Arousal       0.92      0.33      0.49       108\n",
      "\n",
      "                 accuracy                           0.76       956\n",
      "                macro avg       0.84      0.60      0.65       956\n",
      "             weighted avg       0.79      0.76      0.74       956\n",
      "\n",
      "\n",
      "Training with n-grams: (2, 2)\n",
      "F1-score average: 0.762871017334203\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.77      0.94      0.85       485\n",
      " Low Valence High Arousal       0.90      0.43      0.58       110\n",
      "  Low Valence Low Arousal       0.74      0.79      0.76       253\n",
      " High Valence Low Arousal       0.98      0.39      0.56       108\n",
      "\n",
      "                 accuracy                           0.78       956\n",
      "                macro avg       0.85      0.64      0.69       956\n",
      "             weighted avg       0.80      0.78      0.76       956\n",
      "\n",
      "\n",
      "Training with n-grams: (3, 3)\n",
      "F1-score average: 0.7504472243531213\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.76      0.94      0.84       485\n",
      " Low Valence High Arousal       0.90      0.35      0.50       110\n",
      "  Low Valence Low Arousal       0.74      0.78      0.76       253\n",
      " High Valence Low Arousal       0.90      0.41      0.56       108\n",
      "\n",
      "                 accuracy                           0.77       956\n",
      "                macro avg       0.83      0.62      0.67       956\n",
      "             weighted avg       0.79      0.77      0.75       956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "for ngram_range in ngram_configs:\n",
    "    print(f\"\\nTraining with n-grams: {ngram_range}\")\n",
    "    \n",
    "    # Use TfidfVectorizer for feature extraction\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_df=0.95, min_df=80)\n",
    "    text_features = vectorizer.fit_transform(X_lyrics).toarray()\n",
    "    \n",
    "    # Combine Audio and Text Features\n",
    "    combined_features = np.hstack([X_audio, text_features])\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=13)\n",
    "\n",
    "    # Create a pipeline with vectorizer and LinearSVC\n",
    "    model = make_pipeline(StandardScaler(with_mean=False), XGBClassifier(objective='multi:softprob', n_estimators=1000, random_state=13))\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"F1-score average: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['High Valence High Arousal', 'Low Valence High Arousal', 'Low Valence Low Arousal', 'High Valence Low Arousal']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Summarize**\n",
    "A Random Forest model utilizing Audio + Lyrics features, transformed using trigram TF-IDF, achieved the highest performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Improve Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TfidfVectorizer for feature extraction\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3, 3), max_df=0.95, min_df=80)\n",
    "text_features = vectorizer.fit_transform(X_lyrics).toarray()\n",
    "    \n",
    "# Combine Audio and Text Features\n",
    "combined_features = np.hstack([X_audio, text_features])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 SMOTE Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=13, sampling_strategy='not majority')\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Using Optuna for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameters to be tuned\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 4)\n",
    "\n",
    "    # Create Random Forest model with suggested hyperparameters\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                    max_depth=max_depth,\n",
    "                                    min_samples_split=min_samples_split,\n",
    "                                    min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    # Evaluate model using cross-validation\n",
    "    score = cross_val_score(model, X_train_resampled, y_train_resampled, n_jobs=-1, cv=5).mean()\n",
    "\n",
    "    return -score  # Negative because Optuna minimizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score average: 0.8566650671135793\n",
      "Classification Report:\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "High Valence High Arousal       0.90      0.89      0.89       485\n",
      " Low Valence High Arousal       0.81      0.92      0.86       110\n",
      "  Low Valence Low Arousal       0.85      0.79      0.82       253\n",
      " High Valence Low Arousal       0.77      0.81      0.79       108\n",
      "\n",
      "                 accuracy                           0.86       956\n",
      "                macro avg       0.83      0.85      0.84       956\n",
      "             weighted avg       0.86      0.86      0.86       956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the best model\n",
    "best_params = study.best_params\n",
    "best_model = RandomForestClassifier(**best_params)\n",
    "best_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    " # Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "    \n",
    "# Evaluate the model\n",
    "print(f\"F1-score average: {f1_score(y_test, y_pred, average='weighted')}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['High Valence High Arousal', 'Low Valence High Arousal', 'Low Valence Low Arousal', 'High Valence Low Arousal']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
